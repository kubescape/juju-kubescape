---
# Source: kubescape-cloud-operator/templates/ks-service-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  labels:
    app: kubescape
  name: kubescape-sa
  namespace: {{ namespace }}
automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/kv-service-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  labels:
    app: kubescape
  name: ks-sa
  namespace: {{ namespace }}
automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/cloudapi-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: ks-cloud-config
  namespace: {{ namespace }}
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  clusterData: |
    {
      "gatewayWebsocketURL": "gateway:8001",
      "gatewayRestURL": "gateway:8002",
      "vulnScanURL": "kubevuln:8080",
      "kubevulnURL": "kubevuln:8080",
      "kubescapeURL": "kubescape:8080",
      "triggerNewImageScan": "false",
      "accountID": "{{ account }}",
      "clusterName": "{{ clusterName }}", 
      "backendOpenAPI": "https://api.armosec.io/api",
      "eventReceiverRestURL": "https://report.armo.cloud",
      "eventReceiverWebsocketURL": "wss://report.armo.cloud",
      "rootGatewayURL": "wss://ens.euprod1.cyberarmorsoft.com/v1/waitfornotification"       
    }
---
# Source: kubescape-cloud-operator/templates/host-scanner-definition-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: host-scanner-definition
  namespace: {{ namespace }}
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  host-scanner-yaml: |-
    apiVersion: v1
    kind: Namespace
    metadata:
      labels:
        app: kubescape-host-scanner
        k8s-app: kubescape-host-scanner
        kubernetes.io/metadata.name: kubescape-host-scanner
        tier: kubescape-host-scanner-control-plane
      name: kubescape-host-scanner
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: host-scanner
      namespace: {{ namespace }}-host-scanner
      labels:
        app: host-scanner
        k8s-app: kubescape-host-scanner
    spec:
      selector:
        matchLabels:
          name: host-scanner
      template:
        metadata:
          labels:
            name: host-scanner
        spec:
          tolerations:
          # this toleration is to have the DaemonDet runnable on master nodes
          # remove it if your masters can't run pods
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            operator: Exists
            effect: NoSchedule
          containers:
          - name: host-sensor
            image: quay.io/armosec/kube-host-sensor:latest
            securityContext:
              privileged: true
              readOnlyRootFilesystem: true
              procMount: Unmasked
            ports:
              - name: scanner # Do not change port name
                hostPort: 7888
                containerPort: 7888
                protocol: TCP
            resources:
              limits:
                cpu: 0.1m
                memory: 200Mi
              requests:
                cpu: 0.1m
                memory: 200Mi
            volumeMounts:
            - mountPath: /host_fs
              name: host-filesystem
            readinessProbe:
              httpGet:
                path: /kernelVersion
                port: 7888
                initialDelaySeconds: 1
                periodSeconds: 1
          terminationGracePeriodSeconds: 120
          dnsPolicy: ClusterFirstWithHostNet
          automountServiceAccountToken: false
          volumes:
          - hostPath:
              path: /
              type: Directory
            name: host-filesystem
          hostNetwork: true
          hostPID: true
          hostIPC: true
---
# Source: kubescape-cloud-operator/templates/ks-recurring-cronjob-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-cronjob-template
  namespace: {{ namespace }}
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: kubescape-scheduler
      namespace: {{ namespace }}
      labels:
        app: kubescape-scheduler
        tier: ks-control-plane
        armo.tier: "kubescape-scan"
    spec:
      schedule: "0 8 * * *"
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "kubescape-scan"
            spec:
              containers:
              - name: kubescape-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: kubescape-scheduler
---
# Source: kubescape-cloud-operator/templates/kubescape-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-config
  namespace: {{ namespace }}
  labels:
    app: kubescape-config
    tier: ks-control-plane  
data:
  config.json: |
    {
      "accountID": "{{ account }}",
      "clusterName": "{{ clusterName }}",
      "clientID": "",
      "secretKey": ""
    }
---
# Source: kubescape-cloud-operator/templates/kubescape-scheduler-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubescape-scheduler
  namespace: {{ namespace }}
  labels:
    app: kubescape-scheduler
    tier: ks-control-plane
data:
  request-body.json: |-
    {"commands":[{"CommandName":"kubescapeScan","args":{"scanV1": {}}}]}
---
# Source: kubescape-cloud-operator/templates/kubevuln-scheduler-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubevuln-scheduler
  namespace: {{ namespace }}
  labels:
    app: kubevuln-scheduler
    tier: ks-control-plane
data:
  request-body.json: |-
    {"commands":[{"commandName":"scan","designators":[{"designatorType":"Attributes","attributes":{}}]}]}
---
# Source: kubescape-cloud-operator/templates/kv-recurring-cronjob-configmap.yaml
kind: ConfigMap 
apiVersion: v1 
metadata:
  name: kubevuln-cronjob-template # TODO: update template name
  namespace: {{ namespace }}
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: kubevuln-scheduler
      namespace: {{ namespace }}
      labels:
        app: kubevuln-scheduler
        tier: ks-control-plane
        armo.tier: "vuln-scan"
    spec:
      schedule: "0 0 * * *" 
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "vuln-scan"
            spec:
              containers:
              - name: kubevuln-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: kubevuln-scheduler
---
# Source: kubescape-cloud-operator/templates/registry-scan-recurring-cronjob-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: {{ namespace }}
  labels:
    app: ks-cloud-config
    tier: ks-control-plane
  name: registry-scan-cronjob-template
data:
  cronjobTemplate: |-
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: registry-scheduler
      namespace: {{ namespace }}
      labels:
        app: registry-scheduler
        tier: ks-control-plane
        armo.tier: "registry-scan"
    spec:
      schedule: "0 0 * * *"
      jobTemplate:
        spec:
          template:
            metadata:
              labels:
                armo.tier: "registry-scan"
            spec:
              containers:
              - name: registry-scheduler
                image: "quay.io/kubescape/http-request:v0.0.14"
                imagePullPolicy: IfNotPresent
                securityContext:
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                  runAsUser: 100
                args: 
                  - -method=post
                  - -scheme=http
                  - -host=operator:4002
                  - -path=v1/triggerAction
                  - -headers="Content-Type:application/json"
                  - -path-body=/home/ks/request-body.json
                volumeMounts:
                  - name: "request-body-volume"
                    mountPath: /home/ks/request-body.json
                    subPath: request-body.json
                    readOnly: true
              restartPolicy: Never
              automountServiceAccountToken: false
              volumes:
                - name: "request-body-volume" # placeholder
                  configMap:
                    name: registry-scheduler
---
# Source: kubescape-cloud-operator/templates/ks-cluster-role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubescape-sa-roles
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "describe", "watch"]

# Host scan daemonset runs in dedicated namespace applied by kubescape at the begining of the scan.
# At the end of the resources collecting stage Kubescape is taking down both the namespace and the daemonset
- apiGroups: ["", "apps"]
  resources: ["namespaces", "daemonsets"]
  verbs: ["*"]
---
# Source: kubescape-cloud-operator/templates/kv-cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ks-sa-roles
  namespace: {{ namespace }}
  labels:
    app: kubescape
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "watch", "list", "describe"]
---
# Source: kubescape-cloud-operator/templates/ks-cluster-role-binding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubescape-sa-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubescape-sa-roles
subjects:
- kind: ServiceAccount
  name: kubescape-sa
  namespace: {{ namespace }}
---
# Source: kubescape-cloud-operator/templates/kv-cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ks-sa-role-binding
  namespace: {{ namespace }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ks-sa-roles
subjects:
- kind: ServiceAccount
  name: ks-sa
  namespace: {{ namespace }}
---
# Source: kubescape-cloud-operator/templates/ks-ns-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-sa-roles
  namespace: {{ namespace }}
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["get", "list", "describe"]
- apiGroups: ["batch"]
  resources: ["cronjobs"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["*"]
---
# Source: kubescape-cloud-operator/templates/ks-ns-role-binding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ks-sa-role-binding
  namespace: {{ namespace }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ks-sa-roles
subjects:
- kind: ServiceAccount
  name: ks-sa
  namespace: {{ namespace }}
---
# Source: kubescape-cloud-operator/templates/gateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway
  namespace: {{ namespace }}
  labels:
    app: gateway
spec:
  type: ClusterIP
  ports:
    - port: 8001
      targetPort: 8001
      protocol: TCP
      name: "websocket"
    - port: 8002
      targetPort: 8002
      protocol: TCP
      name: "http"
  selector:
    app: gateway
---
# Source: kubescape-cloud-operator/templates/kubescape-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubescape
  namespace: {{ namespace }}
  labels:
    app: kubescape
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: kubescape
---
# Source: kubescape-cloud-operator/templates/kubevuln-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubevuln
  namespace: {{ namespace }}
  labels:
    app: kubevuln
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: "vuln-scan-port"
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: "readiness-port"
  selector:
    app: kubevuln
---
# Source: kubescape-cloud-operator/templates/operator-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: operator
  namespace: {{ namespace }}
  labels:
    app: operator
spec:
  type: ClusterIP
  ports:
    - port: 4002
      targetPort: 4002
      protocol: TCP
  selector:
    app: operator
---
# Source: kubescape-cloud-operator/templates/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: charmed
    app: gateway
    tier: ks-control-plane
    helm.sh/chart: kubescape-cloud-operator-1.8.7
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: gateway
      app.kubernetes.io/instance: charmed
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gateway
        app.kubernetes.io/instance: charmed
        helm.sh/chart: kubescape-cloud-operator-1.8.7
        tier: ks-control-plane
        app: gateway
        helm.sh/revision: "1"
    spec:
      containers:
        - name: gateway
          image: "quay.io/kubescape/gateway:v0.0.14"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100            
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          ports:
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
            - name: "websocket"
              containerPort: 8001
              protocol: TCP
            - name: "rest-api"
              containerPort: 8002
              protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 10Mi
          env:  
            - name: WEBSOCKET_PORT
              value: "8001"
            - name: HTTP_PORT
              value: "8002"
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
          - name: ks-cloud-config
            mountPath: /etc/config
            readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      automountServiceAccountToken: false
---
# Source: kubescape-cloud-operator/templates/kubescape-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubescape
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: kubescape
    app.kubernetes.io/instance: charmed
    app: kubescape
    tier: ks-control-plane
    helm.sh/chart: kubescape-cloud-operator-1.8.7
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kubescape
      app.kubernetes.io/instance: charmed
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kubescape
        app.kubernetes.io/instance: charmed
        helm.sh/chart: kubescape-cloud-operator-1.8.7
        tier: ks-control-plane
        app: kubescape
        helm.sh/revision: "1"
    spec:
      containers:
      - name: kubescape
        image: "quay.io/kubescape/kubescape:v2.0.176"
        imagePullPolicy: "Always"
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 100
        ports:
          - name: http
            containerPort: 8080
            protocol: TCP
        livenessProbe:
          httpGet:
            path: /livez
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 3
        env:
        - name: KS_DOWNLOAD_ARTIFACTS  # When set to true the artifacts will be downloaded every scan execution
          value: "true" 
        - name: KS_DEFAULT_CONFIGMAP_NAME
          value: "kubescape-config"
        - name: KS_DEFAULT_CONFIGMAP_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KS_ENABLE_HOST_SCANNER
          value: "true"
        - name: KS_SUBMIT
          value: "true"
        - name: KS_SKIP_UPDATE_CHECK
          value: "false"
        - name: KS_SAAS_ENV
        
          value: ""
        command:
        - ksserver
        resources:
              limits:
                cpu: 500m
                memory: 500Mi
              requests:
                cpu: 250m
                memory: 256Mi            
        volumeMounts:
        - name: kubescape-config-volume
          mountPath: /home/ks/.kubescape/config.json
          subPath: config.json
        - name: host-scanner-definition
          mountPath: /home/ks/.kubescape/host-scanner.yaml
          subPath: host-scanner-yaml
      serviceAccountName: kubescape-sa
      automountServiceAccountToken: true
      volumes:
      - name: kubescape-config-volume
        configMap:
          name: kubescape-config
      - name: host-scanner-definition
        configMap:
          name: host-scanner-definition
---
# Source: kubescape-cloud-operator/templates/kubevuln-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubevuln
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: kubevuln
    app.kubernetes.io/instance: charmed
    app: kubevuln
    tier: ks-control-plane
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kubevuln
      app.kubernetes.io/instance: charmed
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kubevuln
        app.kubernetes.io/instance: charmed
        helm.sh/chart: kubescape-cloud-operator-1.8.7
        tier: ks-control-plane
        app: kubevuln
        helm.sh/revision: "1"
    spec:
      containers:
        - name: kubevuln
          image: "quay.io/kubescape/kubevuln:v0.0.45"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
          - name: "vuln-scan-port"
            containerPort: 8080
            protocol: TCP
          - name: "readiness-port"
            containerPort: 8000
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
          resources:
            limits:
              cpu: 1500m
              memory: 5000Mi
            requests:
              cpu: 300m
              ephemeral-storage: 5Gi
              memory: 2500Mi
          env:
            - name: PRINT_POST_JSON
              value: ""
            - name: CA_MAX_VULN_SCAN_ROUTINES
              value: "1"
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: operator
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: operator
    app.kubernetes.io/instance: charmed
    app: operator
    tier: ks-control-plane
spec:
  replicas: 1
  revisionHistoryLimit: 2
  strategy:
    rollingUpdate:
      maxSurge: 0%
      maxUnavailable: 100%
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: operator
      app.kubernetes.io/instance: charmed
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: operator
        app.kubernetes.io/instance: charmed
        helm.sh/chart: kubescape-cloud-operator-1.8.7
        tier: ks-control-plane
        app: operator
        helm.sh/revision: "1"
    spec:
      containers:
        - name: operator
          image: "quay.io/kubescape/operator:v0.0.64"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
            - name: "trigger-port"
              containerPort: 4002
              protocol: TCP
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          resources:
            limits:
              cpu: 300m
              memory: 300Mi
            requests:
              cpu: 50m
              memory: 100Mi
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          args:
            - -alsologtostderr
            - -v=4
            - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/kollector-statefulset.yaml
apiVersion: apps/v1
# statefulset is needed in order to avoid two pods reporting from the same cluster in parallel.
# parallel reporting will cause Kubescape SaaS to miss identify the cluster liveness status
kind: StatefulSet
metadata:
  name: kollector
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: kollector
    app.kubernetes.io/instance: charmed
    app: kollector
    tier: ks-control-plane
spec:
  serviceName: ""
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kollector
      app.kubernetes.io/instance: charmed
      tier: ks-control-plane
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kollector
        app.kubernetes.io/instance: charmed
        tier: ks-control-plane
        app: kollector
        helm.sh/chart: kubescape-cloud-operator-1.8.7
        helm.sh/revision: "1"
    spec:
      containers:
        - name: kollector
          image: "quay.io/kubescape/kollector:v0.0.27"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 100
          ports:
            - name: "readiness-port"
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /v1/liveness
              port: readiness-port
            initialDelaySeconds: 3
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /v1/readiness
              port: readiness-port
            initialDelaySeconds: 10
            periodSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 10m
              memory: 40Mi
          env:
            - name: ACTIVATE_CVE_SCAN_ON_NEW_IMAGE_FEATURE
              value: "false"
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: PRINT_REPORT
              value: "false"
          args:
          - -alsologtostderr
          - -v=4
          - 2>&1
          volumeMounts:
            - name: ks-cloud-config
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: ks-cloud-config
          configMap:
            name: ks-cloud-config
            items:
            - key: "clusterData"
              path: "clusterData.json"
      serviceAccountName: ks-sa
      automountServiceAccountToken: true
---
# Source: kubescape-cloud-operator/templates/kubescape-scheduler-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kubescape-scheduler
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: kubescape-scheduler
    app: kubescape-scheduler
    tier: ks-control-plane
    armo.tier: "kubescape-scan"
spec:
  schedule: "0 8 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: kubescape-scheduler
            app: kubescape-scheduler
            armo.tier: "kubescape-scan"
        spec:
          containers:
          - name: kubescape-scheduler
            image: "quay.io/kubescape/http-request:v0.0.14"
            imagePullPolicy: IfNotPresent
            args: 
              - -method=post
              - -scheme=http
              - -host=operator:4002
              - -path=v1/triggerAction
              - -headers="Content-Type:application/json"
              - -path-body=/home/ks/request-body.json
            volumeMounts:
              - name: kubescape-scheduler
                mountPath: /home/ks/request-body.json
                subPath: request-body.json
                readOnly: true
          restartPolicy: Never
          automountServiceAccountToken: false
          volumes:
          - name: kubescape-scheduler
            configMap:
              name: kubescape-scheduler
---
# Source: kubescape-cloud-operator/templates/kubevuln-scheduler-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kubevuln-scheduler
  namespace: {{ namespace }}
  labels:
    app.kubernetes.io/name: kubevuln-scheduler
    app: kubevuln-scheduler
    tier: ks-control-plane
    armo.tier: "vuln-scan"
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app.kubernetes.io/name: kubevuln-scheduler
            app: kubevuln-scheduler
            armo.tier: "vuln-scan"
        spec:
          containers:
          - name: kubevuln-scheduler
            image: "quay.io/kubescape/http-request:v0.0.14"
            imagePullPolicy: IfNotPresent
            args: 
              - -method=post
              - -scheme=http
              - -host=operator:4002
              - -path=v1/triggerAction
              - -headers="Content-Type:application/json"
              - -path-body=/home/ks/request-body.json
            volumeMounts:
              - name: kubevuln-scheduler
                mountPath: /home/ks/request-body.json
                subPath: request-body.json
                readOnly: true
          restartPolicy: Never
          automountServiceAccountToken: false
          volumes:
          - name: kubevuln-scheduler
            configMap:
              name: kubevuln-scheduler
